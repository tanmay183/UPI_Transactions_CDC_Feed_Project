{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b34aeb32-cdc0-42f3-bff6-8ac11beacfda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table 'incremental_load.default.raw_upi_transactions_v1' created with CDC enabled.\n"
     ]
    }
   ],
   "source": [
    "# Create the raw Delta table with CDC enabled\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS incremental_load.default.raw_upi_transactions_v1 (\n",
    "    transaction_id STRING,\n",
    "    upi_id STRING,\n",
    "    merchant_id STRING,\n",
    "    transaction_amount DOUBLE,\n",
    "    transaction_timestamp TIMESTAMP,\n",
    "    transaction_status STRING\n",
    ")\n",
    "USING DELTA\n",
    "TBLPROPERTIES ('delta.enableChangeDataFeed' = true)\n",
    "\"\"\")\n",
    "print(\"Delta table 'incremental_load.default.raw_upi_transactions_v1' created with CDC enabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce3d3b9a-5657-4824-8a70-8fcef9fb6295",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processed successfully.\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "import time\n",
    "\n",
    "# Mock data batches to simulate CDC\n",
    "mock_batches = [\n",
    "    # Batch 1: Insert new transactions\n",
    "    spark.createDataFrame([\n",
    "        (\"T001\", \"upi1@bank\", \"M001\", 500.0, \"2024-12-21 10:00:00\", \"initiated\"),\n",
    "        (\"T002\", \"upi2@bank\", \"M002\", 1000.0, \"2024-12-21 10:05:00\", \"initiated\"),\n",
    "        (\"T003\", \"upi3@bank\", \"M003\", 1500.0, \"2024-12-21 10:10:00\", \"initiated\"),\n",
    "    ], [\"transaction_id\", \"upi_id\", \"merchant_id\", \"transaction_amount\", \"transaction_timestamp\", \"transaction_status\"]),\n",
    "\n",
    "    # Batch 2: Update and insert transactions\n",
    "    spark.createDataFrame([\n",
    "        (\"T001\", \"upi1@bank\", \"M001\", 500.0, \"2024-12-21 10:15:00\", \"completed\"),  # Update transaction\n",
    "        (\"T002\", \"upi2@bank\", \"M002\", 1000.0, \"2024-12-21 10:20:00\", \"failed\"),    # Update transaction\n",
    "        (\"T004\", \"upi4@bank\", \"M004\", 2000.0, \"2024-12-21 10:25:00\", \"initiated\"), # New transaction\n",
    "    ], [\"transaction_id\", \"upi_id\", \"merchant_id\", \"transaction_amount\", \"transaction_timestamp\", \"transaction_status\"]),\n",
    "\n",
    "    # Batch 3: Handle refunds and updates\n",
    "    spark.createDataFrame([\n",
    "        (\"T001\", \"upi1@bank\", \"M001\", 500.0, \"2024-12-21 10:30:00\", \"refunded\"),  # Refund issued\n",
    "        (\"T003\", \"upi3@bank\", \"M003\", 1500.0, \"2024-12-21 10:35:00\", \"completed\"), # Completed transaction\n",
    "    ], [\"transaction_id\", \"upi_id\", \"merchant_id\", \"transaction_amount\", \"transaction_timestamp\", \"transaction_status\"]),\n",
    "]\n",
    "\n",
    "\n",
    "# Merge logic\n",
    "def merge_to_delta_table(delta_table_name: str, batch_df):\n",
    "    delta_table = DeltaTable.forName(spark, delta_table_name)\n",
    "\n",
    "    # Perform merge operation\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        batch_df.alias(\"source\"),\n",
    "        \"target.transaction_id = source.transaction_id\"\n",
    "    ).whenMatchedUpdate(\n",
    "        set={\n",
    "            \"upi_id\": \"source.upi_id\",\n",
    "            \"merchant_id\": \"source.merchant_id\",\n",
    "            \"transaction_amount\": \"source.transaction_amount\",\n",
    "            \"transaction_timestamp\": \"source.transaction_timestamp\",\n",
    "            \"transaction_status\": \"source.transaction_status\"\n",
    "        }\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "# for i, batch_df in enumerate(mock_batches):\n",
    "#     print(f\"Processing batch {i + 1}\")\n",
    "#     merge_to_delta_table(\"incremental_load.default.raw_upi_transactions_v1\", batch_df)\n",
    "#     print(f\"Batch {i + 1} processed successfully.\")\n",
    "#     time.sleep(10)\n",
    "\n",
    "\n",
    "merge_to_delta_table(\"incremental_load.default.raw_upi_transactions_v1\", mock_batches[0])\n",
    "print(f\"Batch processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe8b4956-5154-4e39-b241-b2156e2e3c29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "upi_merchant_pay_trx_mock_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}